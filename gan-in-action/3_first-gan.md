# 첫 번째 GAN 구현하기

**이번 챕터에서 중요한 점**

- **판별자의 목적**은 가능한 한 정확하게 구분하는 것, **생성자의 목적**은 가짜를 진짜처럼 만드는 것
- 생성자를 학습하는 동안 판별자의 파라미터는 변하지 않는다. 반대도 동일하다.



생성자와 판별자는 각자 자신만의 비용 함수를 가진다. 다만 두가지 모두 판별자의 결과를 이용한다.



계속 말하겠지만 **판별자**는 진짜를 진짜로, 가짜를 가짜로 판별하고자 하고 **생성자**는 생성한 가짜를 진짜로 판별되기를 원한다. 조금 더 기술적으로 말하면 **생성자**의 목적은 훈련 데이터셋의 데이터 분포를 흉내내는 샘플을 생성하는 것이다.



**GAN에서 각 네트워크는 자신의 가중치와 절편만 튜닝할 수 있다.** 너무도 당연한 내용이지만 아주 중요한 내용이라 생각된다.



또 다시 말하지만 ㅎㅎ.. >

**판별자의 목적**은 가능한 한 정확하게 구분하는 것이다. 진짜 샘플 $x$ 일 경우 $D(x)$ 는 가능한 한 1(양성 클래스의 레이블)과 가까워야한다. 또 가짜 샘플  $x^{*}$일 경우 $D(x^{})$*는 가능한 한 0(음성 클래스)와 가까워야 한다.

**생성자의 목적**은 이와 반대이다. 훈련 세트에 있는 진짜 샘플과 구분하기 힘든 가짜 샘플 $x^{x}$를 만들어 판별자를 속야아 한다. 수학적으로 생성자는 $D(x^{*})$이 가능한 1이 되도록 가짜 샘플 $x^{x}$를 만들어야 한다.



## 훈련

### 단계 1: 판별자 훈련

1. 랜덤한 진짜 샘플의 미니배치 $x$를 받는다.
2. 랜덤한 잡음 벡터 $z$의 미니배치를 받고 가짜 샘플의 미니배치를 생성한다. $G(z) = x^{*}$
3. $D(x)$와 $D(x^{*})$에 대한 분류 손실을 계산하고 전체 오차를 역전파하여 분류 손실이 최소화되도록 $\theta^{(D)}$를 업데이트 한다.

### 단계2: 생성자 훈련

1. 랜덤한 잡음 벡터 $z$의 미니배치를 받고 가짜 샘플의 미니배치를 생성한다. $G(z) = x^{*}$
2. $D(x^{*})$에 대한 분류 손실을 계산하고 오차를 역전파하여 이 손실을 최대화되도록 $\theta^{(G)}$를 업데이트한다.



이때 또한 중요한 것은 단계 1에서 판별자를 훈련하는 동안 생성자의 파라미터는 변경하지 않는다. 또한 단계 2에서 생성자를 훈련하는 동안 판별자의 파라미터는 변경하지 않는다.





